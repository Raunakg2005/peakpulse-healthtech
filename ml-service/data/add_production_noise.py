"""
Add Realistic Production Noise to Synthetic Data
Makes "too perfect" models realistic for production (target: 80-85% accuracy)
"""

import pandas as pd
import numpy as np
import argparse

def add_realistic_imperfections(features_df, noise_level=0.15):
    """
    Add realistic noise to make data production-grade.
    
    Args:
        features_df: Perfect synthetic features
        noise_level: Proportion of data to corrupt (0.15 = 15%)
    
    Returns:
        Noisy features that should achieve 80-85% accuracy
    """
    df = features_df.copy()
    
    print(f"\n{'='*60}")
    print(f"ADDING PRODUCTION NOISE (Level: {noise_level*100:.0f}%)")
    print(f"{'='*60}")
    
    original_distribution = df['user_type'].value_counts()
    print(f"\nüìä Original Distribution:")
    print(original_distribution)
    
    # === 1. LABEL NOISE (Most Important!) ===
    # Real users don't fit neat categories - 15% are misclassified
    print(f"\n1Ô∏è‚É£ Adding {noise_level*100:.0f}% label noise...")
    num_to_flip = int(len(df) * noise_level)
    flip_indices = np.random.choice(len(df), num_to_flip, replace=False)
    
    # Flip to random labels (simulating real-world ambiguity)
    df.loc[flip_indices, 'user_type'] = np.random.choice(
        ['dropout', 'struggling', 'moderate', 'highly_engaged'], 
        num_to_flip
    )
    print(f"   ‚úì Flipped {num_to_flip} labels randomly")
    
    # === 2. FEATURE OVERLAP (Class Boundaries Aren't Clear) ===
    print(f"\n2Ô∏è‚É£ Adding feature overlap...")
    # Some dropouts look like engaged users and vice versa
    overlap_noise = np.random.normal(0, 0.25, len(df))
    
    for col in ['activity_slope', 'three_day_decline', 'consistency_score', 'momentum']:
        if col in df.columns:
            df[col] = df[col] + overlap_noise
            # Clip to reasonable ranges
            if col == 'consistency_score':
                df[col] = df[col].clip(0, 1)
    
    print(f"   ‚úì Added noise to temporal features")
    
    # === 3. TEMPORAL AMBIGUITY (Patterns Aren't Always Predictive) ===
    print(f"\n3Ô∏è‚É£ Adding temporal ambiguity...")
    
    # Check if temporal features exist
    has_temporal = 'activity_slope' in df.columns
    
    if has_temporal:
        # Some users recover after decline (false alarm for dropout prediction)
        recovery_mask = (df['activity_slope'] < -0.05) & (np.random.random(len(df)) > 0.75)
        num_recovery = recovery_mask.sum()
        df.loc[recovery_mask, 'user_type'] = 'moderate'
        print(f"   ‚úì {num_recovery} users recovered after decline (false dropouts)")
        
        # Some users suddenly drop despite looking good (sudden life events)
        sudden_drop_mask = (df['activity_slope'] > 0) & (np.random.random(len(df)) > 0.92)
        num_sudden = sudden_drop_mask.sum()
        df.loc[sudden_drop_mask, 'user_type'] = 'dropout'
        print(f"   ‚úì {num_sudden} users suddenly dropped (unexpected)")
    else:
        print(f"   ‚ö†Ô∏è Temporal features not found, skipping temporal ambiguity")
    
    # === 4. MEASUREMENT ERROR (Real Apps Aren't Perfect) ===
    print(f"\n4Ô∏è‚É£ Adding measurement errors...")
    measurement_cols = ['avg_steps_last_7_days', 'avg_sleep_hours', 'avg_meditation_minutes']
    
    for col in measurement_cols:
        if col in df.columns:
            # 5-10% measurement error
            measurement_error = np.random.normal(0, 0.08, len(df))
            df[col] = df[col] * (1 + measurement_error)
            df[col] = df[col].clip(0)  # Can't be negative
    
    print(f"   ‚úì Added 5-10% sensor/tracking errors")
    
    # === 5. MISSING PATTERN DATA (Some Users Are Just Random) ===
    print(f"\n5Ô∏è‚É£ Adding random user behavior...")
    # 5% of users are just random - no pattern at all
    random_users = np.random.choice(len(df), int(len(df) * 0.05), replace=False)
    
    for idx in random_users:
        # Randomize their features
        df.loc[idx, 'activity_slope'] = np.random.uniform(-0.1, 0.1)
        df.loc[idx, 'three_day_decline'] = np.random.uniform(-0.3, 0.7)
        df.loc[idx, 'consistency_score'] = np.random.uniform(0.5, 1.0)
        df.loc[idx, 'user_type'] = np.random.choice(
            ['dropout', 'struggling', 'moderate', 'highly_engaged']
        )
    
    print(f"   ‚úì {len(random_users)} users show random behavior")
    
    # === 6. CLASS IMBALANCE SHIFT (Real-World Drift) ===
    # In production, distributions shift over time
    # Slightly skew towards more struggling/moderate users
    print(f"\n6Ô∏è‚É£ Adding distribution drift...")
    
    highly_engaged_mask = df['user_type'] == 'highly_engaged'
    shift_to_moderate = highly_engaged_mask & (np.random.random(len(df)) > 0.85)
    df.loc[shift_to_moderate, 'user_type'] = 'moderate'
    print(f"   ‚úì Shifted {shift_to_moderate.sum()} users from highly_engaged to moderate")
    
    # === SUMMARY ===
    new_distribution = df['user_type'].value_counts()
    print(f"\nüìä New Distribution:")
    print(new_distribution)
    
    print(f"\n{'='*60}")
    print(f"‚úÖ PRODUCTION NOISE ADDED")
    print(f"{'='*60}")
    print(f"\nExpected Impact:")
    print(f"  Perfect Model Accuracy: ~97%")
    print(f"  Noisy Model Accuracy:   ~80-85% (realistic!)")
    print(f"  Difference:             ~12-17% (cost of real-world)")
    
    return df


def compare_datasets(perfect_df, noisy_df):
    """Compare perfect vs noisy datasets."""
    print(f"\n{'='*60}")
    print(f"DATASET COMPARISON")
    print(f"{'='*60}")
    
    print("\nüîç Feature Statistics Comparison:")
    print(f"\n{'Metric':<30} {'Perfect':<15} {'Noisy':<15}")
    print("-"*60)
    
    for col in ['activity_slope', 'three_day_decline', 'consistency_score']:
        if col in perfect_df.columns:
            perfect_mean = perfect_df[col].mean()
            noisy_mean = noisy_df[col].mean()
            print(f"{col:<30} {perfect_mean:<15.4f} {noisy_mean:<15.4f}")
    
    print("\nüìä Label Distribution Comparison:")
    print(f"\n{'User Type':<20} {'Perfect':<15} {'Noisy':<15} {'Change':<10}")
    print("-"*60)
    
    for user_type in ['dropout', 'struggling', 'moderate', 'highly_engaged']:
        perfect_count = (perfect_df['user_type'] == user_type).sum()
        noisy_count = (noisy_df['user_type'] == user_type).sum()
        change = noisy_count - perfect_count
        print(f"{user_type:<20} {perfect_count:<15} {noisy_count:<15} {change:+<10}")


def main():
    parser = argparse.ArgumentParser(description='Add production noise to synthetic data')
    parser.add_argument('--noise_level', type=float, default=0.15, 
                       help='Noise level (0.15 = 15%)')
    parser.add_argument('--input', type=str, default='./ml-service/data/features.csv',
                       help='Input features file')
    parser.add_argument('--output', type=str, default='./ml-service/data/features_noisy.csv',
                       help='Output noisy features file')
    
    args = parser.parse_args()
    
    print(f"Loading perfect data from {args.input}...")
    perfect_df = pd.read_csv(args.input)
    
    # Add noise
    noisy_df = add_realistic_imperfections(perfect_df, noise_level=args.noise_level)
    
    # Compare
    compare_datasets(perfect_df, noisy_df)
    
    # Save noisy version
    noisy_df.to_csv(args.output, index=False)
    print(f"\n‚úì Saved noisy data to {args.output}")
    
    # Also save a copy of perfect data for comparison
    perfect_backup = args.output.replace('_noisy.csv', '_perfect.csv')
    perfect_df.to_csv(perfect_backup, index=False)
    print(f"‚úì Saved perfect data backup to {perfect_backup}")
    
    print(f"\n{'='*60}")
    print(f"NEXT STEPS:")
    print(f"{'='*60}")
    print(f"1. Train on perfect data:  python training/train_models.py")
    print(f"2. Copy noisy to features: cp {args.output} ./ml-service/data/features.csv")
    print(f"3. Train on noisy data:    python training/train_models.py")
    print(f"4. Compare results and present BOTH to judges!")


if __name__ == "__main__":
    main()
